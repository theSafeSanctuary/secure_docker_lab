
networks:
  # We define 5 distinct networks. 
  # We manually specify the subnet CIDR to ensure we can predict IP addresses.
  # This predictability is required for our static firewall rules.
  net_ssh:
    ipam:
      config:
        - subnet: 172.20.10.0/24
  net_av:
    ipam:
      config:
        - subnet: 172.20.20.0/24
  net_falco:
    ipam:
      config:
        - subnet: 172.20.30.0/24
  net_cribl:
    ipam:
      config:
        - subnet: 172.20.40.0/24
  net_sftp:
    ipam:
      config:
        - subnet: 172.20.50.0/24

volumes:
  sftp_data: # Shared storage: SFTP writes here, ClamAV reads from here.
  cribl_config:
  cribl_data:

services:
  # ---------------------------------------------------------
  # FIREWALL (Next-Gen: Router + IDS + Proxy)
  # ---------------------------------------------------------
  firewall:
    build: ./firewall
    container_name: firewall
    hostname: firewall
    # CRITICAL: These capabilities allow the container to modify the kernel's
    # network stack (iptables) and manage process priority (Suricata).
    cap_add:
      - NET_ADMIN
      - NET_RAW
      - SYS_NICE # Required for Suricata
    # CRITICAL: This flips the kernel switch that allows this container to act as a router.
    # Without this, packets arriving at eth1 (SSH Net) destined for eth2 (SFTP Net) would be dropped.
    sysctls:
      - net.ipv4.ip_forward=1
    volumes:
      - ./keys/id_rsa.pub:/tmp/id_rsa.pub:ro # Inject Public Key for SSH access
    # The firewall is the ONLY container attached to ALL networks.
    # It acts as the router between them.
    networks:
      net_ssh:
        ipv4_address: 172.20.10.254
      net_av:
        ipv4_address: 172.20.20.254
      net_falco:
        ipv4_address: 172.20.30.254
      net_cribl:
        ipv4_address: 172.20.40.254
      net_sftp:
        ipv4_address: 172.20.50.254
    ports:
      - "2222:22" # SSH Bastion that Maps host port 2222 to container port 22 for management access.
    restart: always

  # ---------------------------------------------------------
  # SSH SERVER
  # ---------------------------------------------------------
  ssh_server:
    build: ./ssh
    container_name: ssh_server
    # NET_ADMIN is required solely to delete the default Docker route
    # and add our custom route through the Firewall container.
    cap_add:
      - NET_ADMIN
    volumes:
      - ./keys/id_rsa.pub:/tmp/id_rsa.pub:ro # Inject Public Key for SSH access
      - ./common_gateway.sh:/usr/local/bin/gateway.sh
    networks:
      net_ssh:
        ipv4_address: 172.20.10.10

  # ---------------------------------------------------------
  # CLAMAV
  # ---------------------------------------------------------
  clamav:
    build: ./clamav
    container_name: clamav
    cap_add:
      - NET_ADMIN
    volumes:
      - ./keys/id_rsa.pub:/tmp/id_rsa.pub:ro # Inject Public Key for SSH access
      - sftp_data:/scandata
      - ./common_gateway.sh:/usr/local/bin/gateway.sh
    networks:
      net_av:
        ipv4_address: 172.20.20.10

  # ---------------------------------------------------------
  # FALCO
  # ---------------------------------------------------------
  falco:
    build: ./falco
    container_name: falco
    privileged: true
    volumes:
      # Mount the shared volume where rules will be stored
      - /falco-rules:/etc/falco/rules.d
      # Standard Falco mounts
      - /var/run/docker.sock:/host/var/run/docker.sock
      - /dev:/host/dev
      - /proc:/host/proc:ro
      - /boot:/host/boot:ro
      - /lib/modules:/host/lib/modules:ro
      # Mount kernel and tracefs so Falco's libbpf can read tracepoints and maps
      - /sys:/sys:ro
      - /sys/kernel/tracing:/sys/kernel/tracing:ro
      - /sys/kernel/debug:/sys/kernel/debug:ro
      - ./common_gateway.sh:/usr/local/bin/gateway.sh
    environment:
      # Tell Falco to look for rules in the shared mount
      - FALCO_RULES_FILES=/etc/falco/falco_rules.yaml,/etc/falco/falco_rules.local.yaml,/etc/falco/rules.d
      # Ensure Falco watches for file changes to hot-reload automatically
      - FALCO_WATCH_CONFIG_FILES=true
    networks:
      net_falco:
        ipv4_address: 172.20.30.10

  # ---------------------------------------------------------
  # FALCOSIDEKICK
  # ---------------------------------------------------------
  falcosidekick:
    build: ./falcosidekick
    container_name: falcosidekick
    cap_add:
      - NET_ADMIN
    environment:
      - SYSLOG_HOST=172.20.40.20
      - SYSLOG_PORT=514
      - SYSLOG_PROTOCOL=tcp
    volumes:
      - ./common_gateway.sh:/usr/local/bin/gateway.sh
    networks:
      net_falco:
        ipv4_address: 172.20.30.11

  # ---------------------------------------------------------
  # CRIBL LEADER
  # ---------------------------------------------------------
  master:
    image: cribl/cribl:latest
    container_name: cribl_leader
    cap_add:
      - NET_ADMIN
    #user: 999:999
    environment:
      - CRIBL_DIST_MODE=leader
      # This instructs the Leader to listen on port 4200 inside the container
      - CRIBL_DIST_LEADER_URL=tcp://${CRIBL_DIST_TOKEN:-criblmaster}@0.0.0.0:4200
      - CRIBL_VOLUME_DIR=/opt/cribl/config-volume
      - http_proxy=http://172.20.20.254:8888 # Point to Tinyproxy on the Firewall
      - https_proxy=http://172.20.20.254:8888 # Point to Tinyproxy on the Firewall
      - no_proxy=localhost,127.0.0.1,your.cribl.master.ip # Exclude local traffic
    # entrypoint: ["/bin/bash", "-c", "ip route replace default via 172.20.40.254 && exec /sbin/entrypoint.sh"]
    volumes:
      # Map the named volumes to the container paths
      - cribl_config:/opt/cribl/config-volume
      - cribl_data:/opt/cribl/local
    networks:
      net_cribl:
        ipv4_address: 172.20.40.10
    ports:
      - "19000:9000"   # UI Port
      - "4200:4200"    # Cluster Communication Port (REQUIRED for Workers)
    
  # ---------------------------------------------------------
  # CRIBL WORKER
  # ---------------------------------------------------------
  worker:
    image: cribl/cribl:latest
    container_name: cribl_worker
    cap_add:
      - NET_ADMIN
    #user: 999:999
    environment:
      - CRIBL_DIST_MODE=worker
      - CRIBL_DIST_WORKER_TAGS=secure-docker-lab # Tag this Worker
      - CRIBL_DIST_MASTER_URL=tcp://criblmaster@172.20.40.10:4200 # Connect to Leader
      - http_proxy=http://172.20.20.254:8888 # Point to Tinyproxy on the Firewall
      - https_proxy=http://172.20.20.254:8888 # Point to Tinyproxy on the Firewall
      - no_proxy=localhost,127.0.0.1,your.cribl.master.ip # Exclude local traffic
    # entrypoint: ["/bin/bash", "-c", "ip route replace default via 172.20.40.254 && exec /sbin/entrypoint.sh"]
    volumes:
      - ./common_gateway.sh:/usr/local/bin/gateway.sh
      # Workers usually don't need persistent config volumes as they pull config from the Leader
    networks:
      net_cribl:
        ipv4_address: 172.20.40.20
    ports:
      - "9000" # Optional: if you want to see the worker UI directly
    
  # ---------------------------------------------------------
  # SFTP SERVER
  # ---------------------------------------------------------
  sftp_server:
    build: ./sftp
    container_name: sftp_server
    cap_add:
      - NET_ADMIN
    volumes:
      - ./keys/id_rsa.pub:/tmp/id_rsa.pub:ro # Inject Public Key
      - sftp_data:/home/sftpuser/upload
      - ./common_gateway.sh:/usr/local/bin/gateway.sh
    networks:
      net_sftp:
        ipv4_address: 172.20.50.10